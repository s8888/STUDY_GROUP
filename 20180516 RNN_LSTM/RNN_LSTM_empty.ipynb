{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情緒分析 - 史丹佛 IMDb 影評資料集\n",
    "\n",
    "IMDb 資料集共有 50,000 筆 \"影評文字\", 分為訓練資料與測試資料各 25,000 筆, 每一筆 \"影評文字\" 都被標記成 \"正面評價\" 或 \"負面評價\". 我們希望能建立一個模型, 經過大量 \"影評文字\" 訓練後, 此模型能用於預測 \"影評文字\" 是 \"正面評價\" 或 \"負面評價\".\n",
    "\n",
    "[原始出處](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料集的內容\n",
    "\n",
    "評論內文\n",
    "* 正面\n",
    "> Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, .... Grim, but intriguing, and frightening.\n",
    "* 負面\n",
    "> \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 觀察資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# 讀入資料集\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "print(\"資料筆數:\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"\\n分類類別:\")\n",
    "print(numpy.unique(y))\n",
    "\n",
    "print(\"\\n字詞數量:\")\n",
    "print(len(numpy.unique(numpy.hstack(X))))\n",
    "\n",
    "print(\"\\n評論平均長度:\")\n",
    "result = [len(x) for x in X]\n",
    "print(\"平均 %.2f 字 (%.2f)\" % (numpy.mean(result), numpy.std(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 讀入資料集\n",
    "\n",
    "這裡限制只選「最常被使用」的前 1 萬字, 評論中的詞若不在這個範圍內的，就當不存在。這是文字分析常會做的事。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_used_words = #FIX_ME\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_used_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 準備輸入的資料\n",
    "\n",
    "看一下經過「前處理」後的資料，準備輸入模型前的樣子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每個數字都代表英文的一個單字。編號方式是資料庫裡所有文字依「出現次數」的排序: 也就是出現頻率越高, 代表的數字就越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 模型輸入的處理\n",
    "\n",
    "因為每篇評論的長度不一，為了訓練方便：\n",
    "\n",
    "* 設輸入文字長度的上限 (250)\n",
    "* 把每段文字都弄成一樣長, 太短的後面補上 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前 10 篇評論的長度\n",
    "for i in range(10):\n",
    "    print(len(x_train[i]), end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# 每篇評論的最大字數\n",
    "review_length = #FIX_ME\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=review_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=review_length)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 打造你的 RNN\n",
    "\n",
    "這裡我們選用 LSTM, 基本上用哪種 RNN 寫法都是差不多的!\n",
    "\n",
    "### 3.1 決定神經網路架構\n",
    "\n",
    "* 先將 10000 維的文字壓到 256 維\n",
    "* 然後設定 LSTM 層的輸出維度\n",
    "* 最後一個 output, 直接用 sigmoid 送出\n",
    "\n",
    "### 3.2 Word Embedding\n",
    "\n",
    "文字我們用 1-hot 表示是很標準的方式, 不過要注意的是, 因為我們指定要 1 萬個字, 所以每個字是用 1 萬維的向量表示! 這一來很浪費記憶空間, 二來字和字間基本上是沒有關係的。我們可以用某種「合理」的方式, 把字壓到比較小的維度, 這些向量又代表某些意思 (比如說兩個字代表的向量角度小表相關程度大) 等等。\n",
    "\n",
    "這聽來很複雜的事叫 \"word embedding\", 而事實上 Keras 會幫我們做。我們只需告訴 Keras 原來最大的數字是多少 (10000), 還有我們打算壓到幾維 (256)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# 文字向量的維度\n",
    "vector_dim = #FIX_ME\n",
    "\n",
    "# LSTM 層輸出的維度\n",
    "num_lstm_cell = #FIX_ME\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, vector_dim))\n",
    "model.add(LSTM(num_lstm_cell))\n",
    "\n",
    "# 最後一層 dense output, 直接用 sigmoid 送出\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 模型目前的架構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=32, \n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 結果\n",
    "\n",
    "### 5.1 分數\n",
    "\n",
    "來看看測試資料的預測分數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('測試資料的 loss:', score[0])\n",
    "print('測試資料正確率:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 儲存結果\n",
    "\n",
    "把模型的學習結果存檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'your_model_name.h5'#FIX_ME\n",
    "\n",
    "model_json = model.to_json()\n",
    "open('imdb_model_architecture.json', \n",
    "     'w').write(model_json)\n",
    "model.save_weights(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資料\n",
    "\n",
    "1. [深度學習 MOOC 檔案](https://github.com/yenlung/Deep-Learning-MOOC)\n",
    "\n",
    "2. [deep-learning-with-keras-notebooks](http://nbviewer.jupyter.org/github/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/1.a-rnn-introduction.ipynb)\n",
    "\n",
    "3. [Predict Sentiment From Movie Reviews Using Deep Learning](https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/)\n",
    "4. [程式扎記 Keras - IMDb 網路電影資料集](http://puremonkey2010.blogspot.tw/2017/09/toolkit-keras-imdb.html)\n",
    "5. [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
